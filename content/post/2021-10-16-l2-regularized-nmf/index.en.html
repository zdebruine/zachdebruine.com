---
title: "Statistical properties of L1- and L2-regularized NMF"
author: "Zach DeBruine"
date: '2021-10-18'
slug: l2-regularized-nmf
categories:
- NMF
- methods
tags:
- NMF
- regularization
- L2
subtitle: An intuitive take on what exactly L1- and L2-regularized NMF actually does
summary: L1- and L2-regularized non-negative matrix factorizations have special properties. Here I show how L1 is a sparsifying regularization that promotes
  a k-means clustering-like model, while L2 is a densifying regularization that promotes
  convergence of all factors towards the first singular vector.
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: yes
---



<div id="key-takeaways" class="section level2">
<h2>Key Takeaways</h2>
<p>For non-negative matrix factorization:</p>
<ul>
<li>L1 and L2 regularization require diagonalization (factorization of the form <span class="math inline">\(A = wdh\)</span>)</li>
<li>L1 is a sparsifying, L2 is densifying</li>
<li>L1 increases angle between factors, L2 decreases angle between factors</li>
<li>L1 penalties cause factors to converge collectively towards a k-means clustering model, L2 penalties cause each factor to converge individually towards the first singular vector</li>
</ul>
</div>
<div id="regularizing-nmf" class="section level2">
<h2>Regularizing NMF</h2>
<p>Regularizations are intended to improve the interpretability or identifiability of linear models. Consider the least squares problem <span class="math inline">\(ax = b\)</span>, for which common regularizations include:</p>
<ul>
<li><strong>L1/LASSO</strong> regularization: absolute shrinkage, penalty subtracted from <span class="math inline">\(b\)</span></li>
<li><strong>L2/Ridge</strong> regularization: convex shrinkage, penalty added to diagonal of <span class="math inline">\(a\)</span></li>
</ul>
<p>In a typical non-negative least squares (NNLS) fit, these regularizations behave usefully. For example, an L1 penalty equal to the maximum value in <span class="math inline">\(b\)</span> will ensure complete sparsity of the solution.</p>
<p>Now consider NMF by alternating least squares. NMF differs from one-off least squares problems in several ways:</p>
<ul>
<li>It is iterative</li>
<li>The initial distribution of the models are unknown (i.e. projection of random factors)</li>
<li>The distribution of a model at a given iteration is dependent on that of the models at all previous iterations</li>
</ul>
<p>Thus, NMF regularizations have a chain effect: a change in one iteration will lead to a change in information and distribution in the next, and so forth. Thus, if the distribution of the model is not controlled after each update, penalties will cause the model to spiral out of control.</p>
</div>
<div id="controlling-nmf-model-distributions-during-updates" class="section level2">
<h2>Controlling NMF model distributions during updates</h2>
<p>NMF minimizes <span class="math inline">\(A = wh\)</span>. The least squares update of <span class="math inline">\(h\)</span>, column <span class="math inline">\(j\)</span> given <span class="math inline">\(A\)</span> and <span class="math inline">\(w\)</span> is:</p>
<p><span class="math display">\[w^Twh_j = w^TA_j\]</span></p>
<p>Correspondingly, the least squares update of <span class="math inline">\(w\)</span>, row <span class="math inline">\(j\)</span>, given <span class="math inline">\(A\)</span> and <span class="math inline">\(h\)</span> is:</p>
<p><span class="math display">\[hh^Tw_j = hA^T_j\]</span>
These equations are in the form <span class="math inline">\(ax = b\)</span>. For instance, in the update of <span class="math inline">\(h\)</span>, <span class="math inline">\(a = w^Tw\)</span> and <span class="math inline">\(b = w^TA_j\)</span>.</p>
<p>For a regularization penalty strictly in the range (0, 1], we want to guarantee that the penalty will be consistent across random NMF restarts, different datasets, and across alternating least squares updates. To guarantee consistent application of the penalty, we need to control the distribution of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>The distribution of a model can be controlled by diagonalizing the NMF model, such that <span class="math inline">\(A = wdh\)</span>, where columns in <span class="math inline">\(w\)</span> and rows in <span class="math inline">\(h\)</span> are scaled to sum to 1 by a scaling diagonal, <span class="math inline">\(d\)</span>. Factors need not scale to 1, it could be any constant value, but 1 provides nice interpretability.</p>
</div>
<div id="diagonalized-nmf-enables-convex-regularization" class="section level2">
<h2>Diagonalized NMF enables convex regularization</h2>
<p>Let’s load the <code>hawaiibirds</code> dataset and factorize the data at several L1 and L2 penalties, with and without model diagonalization, also calculating various statistics such as sparsity, similarity to k-means clustering, and similarity to the first singular vector.</p>
<pre class="r"><code># devtools::install_github(&quot;zdebruine/RcppML&quot;)
library(RcppML)
data(hawaiibirds)
A &lt;- hawaiibirds$counts</code></pre>
<pre class="r"><code>alphas &lt;- c(c(1, 3, 5, 9) %o% 10^(-3:-1)) # c(seq(0, 0.1, 0.005), seq(0.11, 0.5, 0.01)) # seq(0, 0.98, 0.02)
seeds &lt;- c(123, 456, 789)
kmeans_centers &lt;- t(kmeans(t(as.matrix(A)), 10)$centers)
svd1 &lt;- nmf(A, 1)@w
df &lt;- data.frame()
for(alpha in alphas){
  for(seed in seeds){
    for(diag in c(FALSE, TRUE)){
      m &lt;- nmf(A, 10, seed = seed, diag = diag)
        for(penalty in c(&quot;L1&quot;, &quot;L2&quot;)){
        m_ &lt;- nmf(A, 10, seed = seed, diag = diag,
                   L1 = ifelse(penalty == &quot;L1&quot;, alpha, 0), 
                   L2 = ifelse(penalty == &quot;L2&quot;, alpha, 0),
                  )
        df &lt;- rbind(df, data.frame(
          &quot;alpha&quot; = alpha,
          &quot;seed&quot; = seed,
          &quot;diag&quot; = diag,
          &quot;penalty&quot; = penalty,
          &quot;sparsity&quot; = sum(m_@w == 0) / prod(dim(m_@w)),
          &quot;robustness&quot; = 1 - bipartiteMatch(1 - cosine(m_@w, m@w))$cost/10,
          &quot;mse&quot; = evaluate(m_, A),
          &quot;mean_angle&quot; = mean(cosine(m_@w)),
          &quot;kmeans&quot; = bipartiteMatch(1 - cosine(kmeans_centers, m_@w))$cost/10,
          &quot;svd1&quot; = sum(cosine(m_@w, svd1))/10,
          &quot;color&quot; = ifelse(penalty == &quot;L1&quot;, alpha^0.25, -alpha^0.25)
        ))      
      }
    }
  }
}
df$penalty &lt;- factor(df$penalty)
df$seed &lt;- factor(df$seed)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-2-1.png" width="768" style="display: block; margin: auto auto auto 0;" /></p>
<p>Takeaways:</p>
<ul>
<li>Diagonal scaling guarantees consistent regularization between independent replicates (compare <strong>a</strong>, <strong>c</strong> with <strong>b</strong>, <strong>d</strong>)</li>
<li>L1 regularization increases sparsity of factor models (<strong>b</strong>) while L2 regularization promotes density of the model (<strong>d</strong>)</li>
<li>L1 = 1 guarantees complete sparsity (<strong>b</strong>) while L2 = 1 guarantees complete density (<strong>d</strong>)</li>
</ul>
<p>We might not have expected that L2 is a densifying factorization. Why is this? L2 convexly shrinks values towards zero, and as such decreases the condition number of <span class="math inline">\(a\)</span>. This means signals will be encouraged to “squash” together, and factors in the resulting model will begin to describe similar signal. As this occurs, the model naturally becomes denser until a point is reached that the objective is minimized (at convergence).</p>
</div>
<div id="properties-of-l1--and-l2-regularized-nmf" class="section level2">
<h2>Properties of L1- and L2-regularized NMF</h2>
<p>Let’s consider how L1 and L2 regularizations affect the robustness of information content of factor models relative to the unregularized equivalent, and how they affect the mean squared error loss of the models.</p>
<p>As a measure of the robustness of information content, we use the mean cost of bipartite matching between L1-regularized and unregularized <span class="math inline">\(w\)</span> models on a cosine similarity matrix.</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto auto auto 0;" /></p>
<p>Notice how the L2 penalties tend to be much harsher than the L1 penalties. However, both penalties cause movement of the model away from the unregularized state.</p>
<p>Within the models themselves, we can examine how similar factors are to one another by measuring the mean cosine angle:</p>
<pre class="r"><code>ggplot(subset(df, diag == TRUE &amp; seed == 123), aes(x = alpha, y = mean_angle, color = penalty)) +
  geom_point() + labs(x = &quot;alpha&quot;, y = &quot;mean cosine angle\nbetween factors&quot;) +
  theme_classic() + theme(aspect.ratio = 1) + scale_x_continuous(trans = &quot;sqrt&quot;) +
  stat_smooth(se = F)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-4-1.png" width="384" style="display: block; margin: auto auto auto 0;" /></p>
<p>We can see that L1 penalty increases the distance between factors, while L2 penalty increases the similarity between factors.</p>
<p>Now let’s take a look at how L1 and L2 penalties affect the sparsity of factors, and also calculate the similarity of these models to a k-means clustering or the first singular vector (given by a rank-1 NMF):</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto auto auto 0;" /></p>
<p>L1 is sparsifying while L2 is densifying.</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto auto auto 0;" /></p>
<p>Here, L1 promotes a k-means clustering model while L2 promotes convergence towards the first singular vector.</p>
</div>
<div id="interpreting-l1--and-l2-regularized-factor-models" class="section level2">
<h2>Interpreting L1- and L2-regularized factor models</h2>
<p>We’ll select regularization parameters for further analysis based on a cosine angle of about 0.25 away from the original model:</p>
<pre class="r"><code>model    &lt;- nmf(A, 10, tol = 1e-6, seed = 123)
model_L1 &lt;- nmf(A, 10, tol = 1e-6, seed = 123, L1 = 0.2)
model_L2 &lt;- nmf(A, 10, tol = 1e-6, seed = 123, L2 = 0.02)</code></pre>
<p>Take a look at the clustering of factors in the <span class="math inline">\(w\)</span> models on UMAP coordinates:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-8-1.png" width="288" style="display: block; margin: auto auto auto 0;" /></p>
<p>Similar information is clearly being captured by each of the models, but let’s see in what way.</p>
<p>We’ll align factors in the regularized models to the unregularized models, and then compare specific factors.</p>
<pre class="r"><code>library(ggrepel)
biplot &lt;- function(model1, model2, factor){
  df &lt;- data.frame(&quot;model1&quot; = model1$w[, factor], &quot;model2&quot; = model2$w[, factor], &quot;label&quot; = rownames(model1$w))
  ggplot(df, aes(x = model1, y = model2, label = label)) + geom_point() + theme_classic() + geom_text_repel(size = 2.5)
}

model_L1 &lt;- align(model_L1, model)
model_L2 &lt;- align(model_L2, model)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-10-1.png" width="576" style="display: block; margin: auto auto auto 0;" /></p>
<p>These are very harsh penalties, so notice how L1 can over-sparsify things, while L2 can generate factors that are so dense the information is hardly specific or informative.</p>
<p>A happy medium for sparsifying (or densifying) regularization certainly exists, and this is an objective hyperparameter that must be determined against the objectives of the analysis. Unfortunately, there is nothing against which to optimize – this appears to be a matter of statistical taste.</p>
</div>
<div id="future-directions" class="section level2">
<h2>Future directions</h2>
<ul>
<li>Effect of L1 and L2 regularizations on factorization rank</li>
<li>Intuition behind one-sided L1 and L2 regularization</li>
<li>Intuition behind combined L1/L2 or one-sided L1 vs. one-sided L2</li>
</ul>
</div>
