---
title: "Multiclass predictive modeling for #TidyTuesday NBER papers"
author: Julia Silge
date: '2021-09-29'
slug: nber-papers
categories:
  - rstats
  - tidymodels
tags:
  - rstats
  - tidymodels
subtitle: ''
summary: "Tune and evaluate a multiclass model with lasso regulariztion for economics working papers."
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(silgelib)
theme_set(theme_plex())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
```


This is the latest in my series of [screencasts](https://juliasilge.com/category/tidymodels/) demonstrating how to use the [tidymodels](https://www.tidymodels.org/) packages, from just getting started to tuning more complex models. Today's screencast walks through how to build, tune, and evaluate a multiclass predictive model with text features and lasso regularization, with this week's [`#TidyTuesday` dataset](https://github.com/rfordatascience/tidytuesday) on NBER working papers. `r emo::ji("bookmark_tabs")`

```{r, echo=FALSE}
blogdown::shortcode("youtube", "fooYB4n-ZfU")
```

</br>

Here is the code I used in the video, for those who prefer reading instead of or in addition to video.

## Explore data

Our modeling goal is to predict the category of [National Bureau of Economic Research working papers](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-09-28/readme.md) from the titles and years of the papers.

```{r}
library(tidyverse)

papers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/papers.csv')
programs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/programs.csv')
paper_authors <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_authors.csv')
paper_programs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_programs.csv')
```

Let's start by joining up these datasets to find the info we need.

```{r}
papers_joined <-
    paper_programs %>%
    left_join(programs) %>%
    left_join(papers) %>%
    filter(!is.na(program_category)) %>%
    distinct(paper, program_category, year, title)

papers_joined %>%
    count(program_category)

```

The papers are in three categories (finance, microeconomics, and macroeconomics) so we'll be training a multiclass predictive model, not a binary classification model as we often see or use.

Let's create one exploratory plot before we move on to modeling.

```{r}
library(tidytext)
library(tidylo)

title_log_odds <- 
    papers_joined %>%
    unnest_tokens(word, title) %>%
    filter(!is.na(program_category)) %>%
    count(program_category, word, sort = TRUE) %>%
    bind_log_odds(program_category, word, n)


title_log_odds %>%
    group_by(program_category) %>%
    slice_max(log_odds_weighted, n = 10) %>%
    ungroup() %>%
    ggplot(aes(log_odds_weighted, 
               fct_reorder(word, log_odds_weighted), 
               fill = program_category)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(vars(program_category), scales = "free_y") +
    labs(x = "Log odds (weighted)", y = NULL)

```

These type of relationships between category and title words are what we want to use in our predictive model.


## Build and tune a model

Let's start our modeling by setting up our "data budget". We'll stratify by our outcome `program_category`.

```{r}
library(tidymodels)

set.seed(123)
nber_split <- initial_split(papers_joined, strata = program_category)
nber_train <- training(nber_split)
nber_test <- testing(nber_split)

set.seed(234)
nber_folds <- vfold_cv(nber_train, strata = program_category)
nber_folds
```

Next, let's set up our feature engineering. We will need to transform our text data into features useful for our model by tokenizing and computing (in this case) tf-idf. Let's also downsample since our dataset is imbalanced, with many more of some of the categories than others.

```{r}
library(themis)
library(textrecipes)

nber_rec <-
    recipe(program_category ~ year + title, data = nber_train) %>%
    step_tokenize(title) %>%
    step_tokenfilter(title, max_tokens = 200) %>%
    step_tfidf(title) %>%
    step_downsample(program_category)

nber_rec
```

Then, let's create our model specification for a lasso model. We need to use a model specification that can handle multiclass data, in this case `multinom_reg()`.

```{r}
multi_spec <- 
    multinom_reg(penalty = tune(), mixture = 1) %>%
    set_mode("classification") %>%
    set_engine("glmnet")

multi_spec
```

Now it's time to put the preprocessing and model together in a `workflow()`.

```{r}
nber_wf <- workflow(nber_rec, multi_spec)
nber_wf
```

Since the lasso regularization `penalty` is a hyperparameter of the model (we can't find the best value from fitting the model a single time), let's tune over a grid of possible `penalty` parameters.

```{r}
nber_grid <- grid_regular(penalty(range = c(-5, 0)), levels = 20)

doParallel::registerDoParallel()
set.seed(2021)
nber_rs <-
    tune_grid(
        nber_wf,
        nber_folds,
        grid = nber_grid
    )

nber_rs
```

This is a pretty fast model to fit, since it is linear. How did it turn out?

```{r}
autoplot(nber_rs)
```


```{r}
show_best(nber_rs)
```

## Choose and evaluate a final model

We could use the numerically best model with `select_best()` by often with regularized models we would rather choose a simpler model within some limits of performance. We can choose using the "one-standard error rule" with `select_by_one_std_err()` and then use `last_fit()` to **fit** one time to the training data and **evaluate** one time to the testing data.

```{r}
final_penalty <- 
    nber_rs %>%
    select_by_one_std_err(metric = "roc_auc", desc(penalty))

final_penalty

final_rs <-
    nber_wf %>%
    finalize_workflow(final_penalty) %>%
    last_fit(nber_split)

final_rs
```

How did our final model perform on the training data?

```{r}
collect_metrics(final_rs)
```

We can visualize the difference in performance across classes with a confusion matrix.

```{r}
collect_predictions(final_rs) %>%
    conf_mat(program_category, .pred_class) %>%
    autoplot()
```

We can also visualize the ROC curves for each class.

```{r}
collect_predictions(final_rs)  %>%
    roc_curve(truth = program_category, .pred_Finance:.pred_Micro) %>%
    ggplot(aes(1 - specificity, sensitivity, color = .level)) +
    geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
    geom_path(size = 1.5, alpha = 0.7) +
    labs(color = NULL) +
    coord_fixed()
```

It looks like the finance and microeconomics papers were easier to identify than the macroeconomics papers.

Finally, we can extract (and save, if we like) the fitted workflow from our results to use for predicting on new data.

```{r}
final_fitted <- extract_workflow(final_rs)
## can save this for prediction later with readr::write_rds()

predict(final_fitted, nber_test[111,], type = "prob")
```

We can even make up new paper titles and see how our model classifies them.

```{r}
predict(final_fitted, tibble(year = 2021, title = "Pricing Models for Corporate Responsibility"), type = "prob")
predict(final_fitted, tibble(year = 2021, title = "Teacher Health and Medicaid Expansion"), type = "prob")
```




