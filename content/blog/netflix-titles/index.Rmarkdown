---
title: "Which #TidyTuesday Netflix titles are movies and which are TV shows?"
author: Julia Silge
date: '2021-04-23'
slug: netflix-titles
categories:
  - rstats
  - tidymodels
tags:
  - rstats
  - tidymodels
subtitle: ''
summary: "Use tidymodels to build features for modeling from Netflix description text, then fit and evaluate a support vector machine model."
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(silgelib)
theme_set(theme_plex())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
```


This is the latest in my series of [screencasts](https://juliasilge.com/category/tidymodels/) demonstrating how to use the [tidymodels](https://www.tidymodels.org/) packages, from just starting out to tuning more complex models with many hyperparameters. Today's screencast walks through how to build features for modeling from text, with this week's [`#TidyTuesday` dataset](https://github.com/rfordatascience/tidytuesday) on Netflix titles. `r emo::ji("tv")`

```{r, echo=FALSE}
blogdown::shortcode("youtube", "XYj8vyK864Y")
```

</br>

Here is the code I used in the video, for those who prefer reading instead of or in addition to video.

## Explore data

Our modeling goal is to predict whether a title on Netflix [is a TV show or a movie based on its description in this week's #TidyTuesday dataset](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-20/readme.md).

Let's start by reading in the data.

```{r}
library(tidyverse)

netflix_titles <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv') 
```

How many titles are there in this data set?

```{r}
netflix_titles %>%
  count(type)
```

What do the descriptions look like? It is always a good idea to actually look at your data before modeling.

```{r}
netflix_titles %>%
  slice_sample(n = 10) %>%
  pull(description)
```

What are the top words in each category?

```{r}
library(tidytext)

netflix_titles %>%
  unnest_tokens(word, description) %>%
  anti_join(get_stopwords()) %>%
  count(type, word, sort = TRUE) %>%
  group_by(type) %>%
  slice_max(n, n = 15) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, n, type)) %>%
  ggplot(aes(n, word, fill = type)) +
  geom_col(show.legend = FALSE, alpha = 0.8) +
  scale_y_reordered() +
  facet_wrap(~type, scales = "free") +
  labs(x = "Word frequency", y = NULL,
       title = "Top words in Netflix descriptions by frequency",
       subtitle = "After removing stop words")
```

There are some differences in even the very top, most frequent words used, so hopefully we can train a model to distinguish these categories.

## Build a model

We can start by loading the tidymodels metapackage, splitting our data into training and testing sets, and creating cross-validation samples. Think about this stage as _spending your data budget_.

```{r}
library(tidymodels)

set.seed(123)
netflix_split <- netflix_titles %>%
  select(type, description) %>%
  initial_split(strata = type)

netflix_train <- training(netflix_split)
netflix_test <- testing(netflix_split)

set.seed(234)
netflix_folds <- vfold_cv(netflix_train, strata = type)
netflix_folds
```

Next, let's create our feature engineering recipe and our model, and then put them together in a modeling [workflow](https://www.tmwr.org/workflows.html). This feature engineering recipe is a good basic default for a text model, but you can read more about creating features from text in my book with Emil Hvitfeldt, [*Supervised Machine Learning for Text Analysis in R*](https://smltar.com/).

```{r}
library(textrecipes)
library(themis)

netflix_rec <- recipe(type ~ description, data = netflix_train) %>%
  step_tokenize(description) %>%
  step_tokenfilter(description, max_tokens = 1e3) %>%
  step_tfidf(description) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_smote(type)

netflix_rec

svm_spec <- svm_linear() %>%
  set_mode("classification") %>%
  set_engine("LiblineaR")

netflix_wf <- workflow() %>%
  add_recipe(netflix_rec) %>%
  add_model(svm_spec)

netflix_wf
```

This linear support vector machine is a newer model in parsnip, currently in the development version on GitHub. Linear SVMs are often a good starting choice for text models. When you use an SVM, remember to `step_normalize()`!

Now let's fit this workflow (that combines feature engineering with the SVM model) to the resamples we created earlier. The linear SVM model does not support class probabilities, so we need to set a custom `metric_set()` that only includes metrics for hard class probabilities.

```{r}
doParallel::registerDoParallel()
set.seed(123)
svm_rs <- fit_resamples(
  netflix_wf,
  netflix_folds,
  metrics = metric_set(accuracy, recall, precision),
  control = control_resamples(save_pred = TRUE)
)

collect_metrics(svm_rs)
```

We can use [`conf_mat_resampled()`](https://tune.tidymodels.org/reference/conf_mat_resampled.html) to compute a separate confusion matrix for each resample, and then average the cell counts.

```{r}
svm_rs %>%
  conf_mat_resampled(tidy = FALSE) %>%
  autoplot()
```


## Fit and evaluate final model

Next, let's fit our model on last time to the **whole** training set at once (rather than resampled data) and evaluate on the testing set. This is the first time we have touched the testing set.

```{r}
final_fitted <- last_fit(
  netflix_wf,
  netflix_split,
  metrics = metric_set(accuracy, recall, precision)
)
collect_metrics(final_fitted)
```

Our performance on the testing set is about the same as what we found with our resampled data, which is good.

We can explore how the model is doing for both the positive and negative classes with a confusion matrix.

```{r}
collect_predictions(final_fitted) %>%
  conf_mat(type, .pred_class)
```

There is a fitted workflow in the `.workflow` column of the `final_fitted` tibble that can be saved and used for prediction later. Here, let's pull out the parsnip fit and learn about model explanations.

```{r}
netflix_fit <- pull_workflow_fit(final_fitted$.workflow[[1]])

tidy(netflix_fit) %>%
  arrange(estimate)
```

Those `term` items are the word features that we created during feature engineering. Let's set up a visualization to see them better.

```{r}
tidy(netflix_fit) %>%
  filter(term != "Bias") %>%
  group_by(sign = estimate > 0) %>%
  slice_max(abs(estimate), n = 15) %>%
  ungroup() %>%
  mutate(term = str_remove(term, "tfidf_description_"),
         sign = if_else(sign, "More from TV shows", "More from movies")) %>%
  ggplot(aes(abs(estimate), fct_reorder(term, abs(estimate)), fill = sign)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~sign, scales = "free") +
  labs(x = "Coefficient from linear SVM", y = NULL,
       title = "Which words are most predictive of movies vs. TV shows?",
       subtitle = "For description text of movies and TV shows on Netflix")
```




